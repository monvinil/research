# Research Verification Standards

## The Problem This Solves

In Cycles 1-2, the engine used hardcoded Python rules to extract signals and generate opportunities. The output *looked* like AI-reasoned research but was template-fill — no actual intelligence behind it. This document establishes how to verify that research output is genuine.

## How to Tell Real Research from Template Output

### Red Flags (Fake/Template Research)
- Signal details that are generic enough to apply to any industry
- Perfectly formatted outputs that never express uncertainty
- Data points that are round numbers or suspiciously clean
- Every opportunity scores between 60-85 (avoiding extremes)
- Kill index stays empty cycle after cycle (nothing is bad enough to kill)
- Signals don't reference each other or build on previous cycles
- "Why now" explanations that just say "AI got better"

### Green Flags (Real Research)
- Signals cite specific data sources with URLs or API endpoints
- Confidence levels vary (some High, some Low) with explanation of why
- Cross-references between signals ("this reinforces signal A-2026-02-10-003")
- Kill index grows — bad ideas are actively killed with specific reasons
- Outputs express genuine uncertainty ("we couldn't verify X, need Y data")
- Numbers are messy/specific (not "$50B market" but "$47.3B with 12% penetration")
- Cycle-over-cycle evolution — patterns strengthen, weaken, or get killed
- Agent B kills opportunities (if everything passes, the filter is broken)

## Verification Checklist (Run After Every Cycle)

### 1. Source Verification
- [ ] Pick 3 random signals. Can you trace each data point to its source?
- [ ] Are the numbers current (not stale data from 6+ months ago)?
- [ ] Do the FRED/BLS data points match what you see on fred.stlouisfed.org?

### 2. Reasoning Verification
- [ ] Read Agent A's signal extraction. Does each signal explain *why* it matters to the thesis, not just *what* happened?
- [ ] Read Agent C's grading. Are scores justified with reasoning, or just numbers?
- [ ] Read Agent B's verifications. Does it name specific incumbents, specific costs, specific regulatory issues?

### 3. Kill Index Health
- [ ] Has anything been killed? (If 5+ cycles pass with zero kills, something is wrong)
- [ ] Are kill reasons specific and structural? ("CPA license required and non-transferable in CA" not "regulatory risk")
- [ ] Do new signals get checked against existing kill patterns?

### 4. Cross-Cycle Integrity
- [ ] Do patterns from previous cycles appear in this cycle's analysis?
- [ ] Are patterns being updated (strengthened/weakened) based on new data?
- [ ] Is the Master synthesis different from last cycle, or copy-paste?

### 5. Honesty Check
- [ ] Does the output say "I don't know" or "couldn't verify" anywhere? (It should)
- [ ] Are there opportunities that scored below 40? (They should exist)
- [ ] Does Agent B express genuine skepticism, or does it validate everything?

## Engine Provenance Markers

Every output file includes metadata showing what produced it:

```json
{
  "engine": "opus-4.6-in-session | opus-4.6-parallel | hardcoded-rules",
  "cycle": N,
  "timestamp": "ISO 8601",
  "agent_model": "which model ran this agent",
  "verification_level": "none | source-checked | cross-referenced | stress-tested"
}
```

- `hardcoded-rules`: Output was generated by Python if/else logic. Not AI-reasoned.
- `opus-4.6-in-session`: Single Claude Code session acting as all agents.
- `opus-4.6-parallel`: Multiple Claude Code sessions, one per agent.

## What the Operator Should Spot-Check

After each cycle, spend 10 minutes on:

1. **Open dashboard.json** — read the `master_synthesis`. Does it sound like it was written about YOUR data, or could it apply to any research engine?

2. **Pick the top-scoring signal** — follow its source. Is the claim real?

3. **Pick the lowest-scoring opportunity** — read Agent B's verdict. Is the skepticism specific and well-reasoned?

4. **Check key_unknowns** — are these genuine gaps, or filler? Genuine gaps should make you think "yeah, I'd need to check that too."

5. **Compare to last cycle** — open the previous cycle's dashboard.json. What changed? If nothing substantive changed despite new data, the engine isn't learning.
